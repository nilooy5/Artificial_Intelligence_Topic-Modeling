{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "131072"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "csv.field_size_limit(1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the file and performing basic cleanup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_18900\\1155373532.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['speech'] = df['speech'].str.replace('\\nAddress on Administration Goals (Budget Message)\\n', '')\n",
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_18900\\1155373532.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'][df['date'] == 'Address on Administration Goals (Budget Message)'] = temp_date.values[0]\n",
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_18900\\1155373532.py:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['speech'] = df['speech'].str.replace('\\\\\\'', '')\n"
     ]
    }
   ],
   "source": [
    "# read csv file from data folder\n",
    "df = pd.read_csv(os.path.join('data', 'state-of-the-union.csv'), names=['year', 'speech'], skiprows=1)\n",
    "\n",
    "df['speech'] = df['speech'].str.replace('\\nState of the Union Address\\n', '')\n",
    "df['speech'] = df['speech'].str.replace('\\nAddress to Joint Session of Congress \\n', '')\n",
    "df['speech'] = df['speech'].str.replace('\\nAddress on Administration Goals (Budget Message)\\n', '')\n",
    "df['speech'] = df['speech'].str.replace('\\nAddress on Administration Goals\\n', '')\n",
    "df['speech'] = df['speech'].str.replace('\\nAddress to Congress \\n', '')\n",
    "\n",
    "df['president'] = df['speech']\n",
    "\n",
    "df['president'] = df['president'].str.split('\\n').str[0]\n",
    "df['date'] = df['speech'].str.split('\\n').str[1]\n",
    "\n",
    "temp_date = df[df['date'] == 'Address on Administration Goals (Budget Message)']['speech'].str.split('\\n').str[3]\n",
    "df['date'][df['date'] == 'Address on Administration Goals (Budget Message)'] = temp_date.values[0]\n",
    "\n",
    "# delete first 3 lines of speech\n",
    "df['speech'] = df['speech'].str.split('\\n').str[3:]\n",
    "# make a string list\n",
    "df['speech'] = df['speech'].str.join(' ')\n",
    "# replace \\ with ''\n",
    "df['speech'] = df['speech'].str.replace('\\\\\\'', '')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performing Lemmatization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in word_tokenize(text)]\n",
    "\n",
    "\n",
    "df['speech'] = df['speech'].apply(lemmatize_text)\n",
    "df['speech'] = df['speech'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df['speech'] = df['speech'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(df['speech'].head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performing Stemming"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellowcitizen senat hou repr meet feel much sa...\n",
      "1    fellowcitizen senat hou repr vain may expect p...\n",
      "2    fellowcitizen senat hou repr abat satisfact me...\n",
      "3    fellowcitizen senat hou repr sinc commenc term...\n",
      "4    fellowcitizen senat hou repr call mind graciou...\n",
      "5    fellowcitizen senat hou repr trust deceiv indu...\n",
      "6    fellowcitizen senat hou repr recur intern situ...\n",
      "7    gentlemen senat gentlemen hou repr wa time app...\n",
      "8    gentlemen senat gentlemen hou repr rever resig...\n",
      "9    gentlemen senat gentlemen hou repr peculiar sa...\n",
      "Name: speech, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]\n",
    "\n",
    "\n",
    "df['speech'] = df['speech'].apply(stem_text)\n",
    "df['speech'] = df['speech'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df['speech'] = df['speech'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# remove punctuation\n",
    "# import string\n",
    "#\n",
    "# df['speech'] = df['speech'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "\n",
    "print(df['speech'].head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellowcitizen senat hous repres  meet feel muc...\n",
      "1    fellowcitizen senat hous repres   vain may exp...\n",
      "2    fellowcitizen senat hous repres  abat satisfac...\n",
      "3    fellowcitizen senat hous repres  sinc commenc ...\n",
      "4    fellowcitizen senat hous repres  call mind gra...\n",
      "5    fellowcitizen senat hous repres  trust deceiv ...\n",
      "6    fellowcitizen senat hous repres  recur intern ...\n",
      "7    gentlemen senat gentlemen hous repres  wa time...\n",
      "8    gentlemen senat gentlemen hous repres  rever r...\n",
      "9    gentlemen senat gentlemen hous repres  peculia...\n",
      "Name: speech, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_18900\\939221554.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['speech'] = df['speech'].str.replace('[{}]'.format(string.punctuation), '')\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "import string\n",
    "\n",
    "df['speech'] = df['speech'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "print(df['speech'].head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [7], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m corpora\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# create a dictionary from a list of speeches\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m dictionary \u001B[38;5;241m=\u001B[39m \u001B[43mcorpora\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDictionary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mspeech\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# convert the dictionary to a bag of words\u001B[39;00m\n\u001B[0;32m      7\u001B[0m corpus \u001B[38;5;241m=\u001B[39m [dictionary\u001B[38;5;241m.\u001B[39mdoc2bow(speech) \u001B[38;5;28;01mfor\u001B[39;00m speech \u001B[38;5;129;01min\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspeech\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "File \u001B[1;32m~\\PycharmProjects\\Aritificial Intelligence\\Assignment 2\\venv\\lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001B[0m, in \u001B[0;36mDictionary.__init__\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_nnz \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m documents \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprune_at\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprune_at\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_lifecycle_event(\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreated\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     81\u001B[0m         msg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_docs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m documents (total \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_pos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m corpus positions)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     82\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Aritificial Intelligence\\Assignment 2\\venv\\lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001B[0m, in \u001B[0;36mDictionary.add_documents\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m    201\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madding document #\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m to \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, docno, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;66;03m# update Dictionary with the document\u001B[39;00m\n\u001B[1;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdoc2bow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_update\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# ignore the result, here we only care about updating token ids\u001B[39;00m\n\u001B[0;32m    206\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m documents (total \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m corpus positions)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_docs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_pos)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Aritificial Intelligence\\Assignment 2\\venv\\lib\\site-packages\\gensim\\corpora\\dictionary.py:241\u001B[0m, in \u001B[0;36mDictionary.doc2bow\u001B[1;34m(self, document, allow_update, return_missing)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;124;03m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \n\u001B[0;32m    211\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    238\u001B[0m \n\u001B[0;32m    239\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(document, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoc2bow expects an array of unicode tokens on input, not a single string\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    243\u001B[0m \u001B[38;5;66;03m# Construct (word, frequency) mapping.\u001B[39;00m\n\u001B[0;32m    244\u001B[0m counter \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mint\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# create a dictionary from a list of speeches\n",
    "dictionary = corpora.Dictionary(df['speech'])\n",
    "\n",
    "# convert the dictionary to a bag of words\n",
    "corpus = [dictionary.doc2bow(speech) for speech in df['speech']]\n",
    "\n",
    "print(corpus[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['value', 'trying', 'copy', 'slice', 'from', 'dataframe', 'caveats', 'documentation']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "tokens = gensim.utils.simple_preprocess(\"A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation\", min_len=4)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
