{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "1000000000"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "csv.field_size_limit(1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the file and performing basic cleanup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\2210526369.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_frame['speech'] = data_frame['speech'].str.replace('\\nAddress on Administration Goals (Budget Message)\\n', '')\n",
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\2210526369.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_frame['date'][data_frame['date'] == 'Address on Administration Goals (Budget Message)'] = temp_date.values[0]\n",
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\2210526369.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_frame['speech'] = data_frame['speech'].str.replace('\\\\\\'', '')\n",
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\2210526369.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_frame['speech'] = data_frame['speech'].str.replace('[{}]'.format(string.punctuation), '')\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# read csv file from data folder\n",
    "df = pd.read_csv(os.path.join('data', 'state-of-the-union.csv'), names=['year', 'speech'], skiprows=1)\n",
    "\n",
    "def perform_initial_cleanup(data_frame):\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('\\nState of the Union Address\\n', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('\\nAddress to Joint Session of Congress \\n', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('\\nAddress on Administration Goals (Budget Message)\\n', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('\\nAddress on Administration Goals\\n', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('\\nAddress to Congress \\n', '')\n",
    "    data_frame['president'] = data_frame['speech']\n",
    "    data_frame['president'] = data_frame['president'].str.split('\\n').str[0]\n",
    "    data_frame['date'] = data_frame['speech'].str.split('\\n').str[1]\n",
    "    temp_date = data_frame[data_frame['date'] == 'Address on Administration Goals (Budget Message)']['speech'].str.split('\\n').str[3]\n",
    "    data_frame['date'][data_frame['date'] == 'Address on Administration Goals (Budget Message)'] = temp_date.values[0]\n",
    "    # delete first 3 lines of speech\n",
    "    data_frame['speech'] = data_frame['speech'].str.split('\\n').str[3:]\n",
    "    # make a string list\n",
    "    data_frame['speech'] = data_frame['speech'].str.join(' ')\n",
    "    # replace \\ with ''\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('\\\\\\'', '')\n",
    "    # decapitalize\n",
    "    data_frame['speech'] = data_frame['speech'].str.lower()\n",
    "    # remove punctuation\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "    # remove word 'America' 'america' 'americas' 'Americas' 'american' 'American' 'Americans' 'American' 'americans'\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('America', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('Americas', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('American', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('Americans', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('america', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('americas', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('american', '')\n",
    "    data_frame['speech'] = data_frame['speech'].str.replace('americans', '')\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "df = perform_initial_cleanup(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performing Lemmatization & Cleanup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "0      [fellowcitizens, senate, house, representative...\n1      [fellowcitizens, senate, house, representative...\n2      [fellowcitizens, senate, house, representative...\n3      [fellowcitizens, senate, house, representative...\n4      [fellowcitizens, senate, house, representative...\n                             ...                        \n220    [madam, speaker, vice, president, cheney, memb...\n221    [madame, speaker, vice, president, member, con...\n222    [madame, speaker, vice, president, biden, memb...\n223    [speaker, vice, president, member, congress, d...\n224    [speaker, vice, president, member, congress, d...\nName: speech, Length: 225, dtype: object"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk import pos_tag\n",
    "\n",
    "# perform initial lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in word_tokenize(text)]\n",
    "\n",
    "\n",
    "df['speech'] = df['speech'].apply(lemmatize_text)\n",
    "df['speech'] = df['speech'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df['speech'] = df['speech'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "NUM_TOPICS = 5\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def lemmatize_sent(text):\n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag))\n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "\n",
    "def clean_lemma_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return lemmatize_sent(' '.join(cleaned_text))\n",
    "\n",
    "\n",
    "# clean the speeches\n",
    "tokenized_speeches = df['speech'].apply(clean_lemma_text)\n",
    "tokenized_speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making LDA & LSI model using Gensim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model of 1990:\n",
      "Topic #0: 0.019*\"must\" + 0.007*\"let\" + 0.007*\"make\" + 0.007*\"new\" + 0.006*\"ask\" + 0.006*\"future\" + 0.005*\"tonight\" + 0.005*\"year\" + 0.005*\"budget\" + 0.005*\"time\"\n",
      "Topic #1: 0.015*\"must\" + 0.008*\"budget\" + 0.008*\"ask\" + 0.007*\"new\" + 0.007*\"tonight\" + 0.006*\"year\" + 0.006*\"let\" + 0.006*\"make\" + 0.006*\"time\" + 0.005*\"future\"\n",
      "Topic #2: 0.011*\"must\" + 0.008*\"new\" + 0.006*\"ask\" + 0.006*\"make\" + 0.005*\"tonight\" + 0.005*\"budget\" + 0.005*\"time\" + 0.005*\"need\" + 0.005*\"year\" + 0.005*\"congress\"\n",
      "Topic #3: 0.009*\"must\" + 0.005*\"new\" + 0.004*\"ask\" + 0.004*\"tonight\" + 0.004*\"year\" + 0.004*\"time\" + 0.003*\"drug\" + 0.003*\"let\" + 0.003*\"nation\" + 0.003*\"future\"\n",
      "Topic #4: 0.015*\"must\" + 0.007*\"ask\" + 0.006*\"budget\" + 0.006*\"let\" + 0.006*\"new\" + 0.005*\"make\" + 0.005*\"year\" + 0.005*\"time\" + 0.005*\"tonight\" + 0.005*\"work\"\n",
      "\n",
      "LSI Model of 1990:\n",
      "Topic #0: 0.411*\"must\" + 0.185*\"new\" + 0.176*\"tonight\" + 0.168*\"make\" + 0.168*\"ask\" + 0.159*\"budget\" + 0.151*\"let\" + 0.151*\"year\" + 0.134*\"time\" + 0.126*\"future\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1991:\n",
      "Topic #0: 0.012*\"every\" + 0.011*\"year\" + 0.010*\"world\" + 0.009*\"time\" + 0.008*\"one\" + 0.007*\"new\" + 0.007*\"make\" + 0.006*\"nation\" + 0.006*\"let\" + 0.005*\"tonight\"\n",
      "Topic #1: 0.009*\"world\" + 0.009*\"every\" + 0.008*\"must\" + 0.007*\"one\" + 0.007*\"time\" + 0.006*\"make\" + 0.006*\"need\" + 0.006*\"idea\" + 0.006*\"freedom\" + 0.006*\"year\"\n",
      "Topic #2: 0.010*\"every\" + 0.010*\"world\" + 0.009*\"one\" + 0.008*\"year\" + 0.008*\"make\" + 0.007*\"tonight\" + 0.006*\"new\" + 0.006*\"must\" + 0.005*\"today\" + 0.005*\"child\"\n",
      "Topic #3: 0.012*\"every\" + 0.009*\"world\" + 0.009*\"must\" + 0.008*\"year\" + 0.008*\"new\" + 0.008*\"one\" + 0.007*\"time\" + 0.007*\"future\" + 0.006*\"nation\" + 0.006*\"today\"\n",
      "Topic #4: 0.007*\"one\" + 0.007*\"every\" + 0.007*\"year\" + 0.007*\"world\" + 0.006*\"make\" + 0.006*\"must\" + 0.005*\"need\" + 0.005*\"time\" + 0.005*\"new\" + 0.005*\"change\"\n",
      "\n",
      "LSI Model of 1991:\n",
      "Topic #0: 0.252*\"every\" + 0.242*\"world\" + 0.212*\"year\" + 0.192*\"one\" + 0.171*\"new\" + 0.161*\"make\" + 0.161*\"must\" + 0.151*\"time\" + 0.131*\"today\" + 0.131*\"tonight\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1992:\n",
      "Topic #0: 0.012*\"world\" + 0.008*\"nation\" + 0.008*\"know\" + 0.006*\"future\" + 0.006*\"people\" + 0.005*\"freedom\" + 0.005*\"community\" + 0.005*\"state\" + 0.005*\"must\" + 0.005*\"power\"\n",
      "Topic #1: 0.009*\"world\" + 0.006*\"nation\" + 0.005*\"freedom\" + 0.005*\"new\" + 0.004*\"every\" + 0.004*\"one\" + 0.004*\"know\" + 0.004*\"community\" + 0.004*\"future\" + 0.004*\"state\"\n",
      "Topic #2: 0.009*\"world\" + 0.006*\"nation\" + 0.006*\"freedom\" + 0.005*\"know\" + 0.005*\"people\" + 0.004*\"future\" + 0.004*\"power\" + 0.004*\"state\" + 0.004*\"make\" + 0.004*\"struggle\"\n",
      "Topic #3: 0.014*\"world\" + 0.008*\"know\" + 0.007*\"nation\" + 0.006*\"one\" + 0.006*\"freedom\" + 0.005*\"must\" + 0.005*\"every\" + 0.005*\"united\" + 0.005*\"community\" + 0.005*\"state\"\n",
      "Topic #4: 0.010*\"world\" + 0.008*\"nation\" + 0.006*\"freedom\" + 0.006*\"people\" + 0.006*\"know\" + 0.006*\"power\" + 0.005*\"future\" + 0.005*\"make\" + 0.005*\"state\" + 0.005*\"community\"\n",
      "\n",
      "LSI Model of 1992:\n",
      "Topic #0: 0.316*\"world\" + 0.207*\"nation\" + 0.196*\"know\" + 0.164*\"freedom\" + 0.153*\"people\" + 0.142*\"power\" + 0.142*\"future\" + 0.142*\"state\" + 0.131*\"one\" + 0.131*\"must\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1993:\n",
      "Topic #0: 0.011*\"people\" + 0.010*\"must\" + 0.009*\"plan\" + 0.009*\"know\" + 0.007*\"help\" + 0.007*\"tax\" + 0.006*\"make\" + 0.006*\"time\" + 0.006*\"country\" + 0.006*\"world\"\n",
      "Topic #1: 0.008*\"people\" + 0.008*\"must\" + 0.006*\"know\" + 0.006*\"plan\" + 0.006*\"ask\" + 0.005*\"time\" + 0.005*\"new\" + 0.005*\"world\" + 0.005*\"help\" + 0.005*\"right\"\n",
      "Topic #2: 0.011*\"must\" + 0.009*\"plan\" + 0.009*\"people\" + 0.008*\"tax\" + 0.008*\"help\" + 0.008*\"country\" + 0.007*\"know\" + 0.007*\"world\" + 0.006*\"new\" + 0.006*\"time\"\n",
      "Topic #3: 0.010*\"plan\" + 0.008*\"must\" + 0.008*\"help\" + 0.007*\"know\" + 0.007*\"people\" + 0.007*\"world\" + 0.006*\"thing\" + 0.006*\"make\" + 0.006*\"right\" + 0.006*\"new\"\n",
      "Topic #4: 0.008*\"plan\" + 0.006*\"must\" + 0.006*\"know\" + 0.006*\"people\" + 0.006*\"world\" + 0.006*\"good\" + 0.005*\"new\" + 0.005*\"time\" + 0.005*\"get\" + 0.005*\"help\"\n",
      "\n",
      "LSI Model of 1993:\n",
      "Topic #0: 0.245*\"must\" + 0.220*\"people\" + 0.212*\"plan\" + 0.203*\"know\" + 0.178*\"help\" + 0.169*\"world\" + 0.161*\"time\" + 0.144*\"ask\" + 0.144*\"new\" + 0.144*\"make\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1994:\n",
      "Topic #0: 0.010*\"must\" + 0.009*\"government\" + 0.009*\"program\" + 0.009*\"people\" + 0.008*\"cut\" + 0.008*\"care\" + 0.008*\"year\" + 0.008*\"work\" + 0.008*\"make\" + 0.007*\"say\"\n",
      "Topic #1: 0.011*\"people\" + 0.008*\"year\" + 0.008*\"new\" + 0.008*\"must\" + 0.007*\"care\" + 0.007*\"government\" + 0.007*\"investment\" + 0.007*\"cut\" + 0.007*\"make\" + 0.006*\"job\"\n",
      "Topic #2: 0.012*\"people\" + 0.008*\"year\" + 0.008*\"new\" + 0.007*\"must\" + 0.007*\"cut\" + 0.007*\"make\" + 0.007*\"government\" + 0.007*\"plan\" + 0.006*\"work\" + 0.006*\"job\"\n",
      "Topic #3: 0.010*\"new\" + 0.009*\"year\" + 0.009*\"people\" + 0.008*\"must\" + 0.008*\"make\" + 0.007*\"say\" + 0.007*\"job\" + 0.007*\"government\" + 0.007*\"work\" + 0.007*\"plan\"\n",
      "Topic #4: 0.012*\"people\" + 0.011*\"year\" + 0.010*\"must\" + 0.008*\"cut\" + 0.008*\"say\" + 0.008*\"work\" + 0.007*\"tax\" + 0.006*\"make\" + 0.006*\"job\" + 0.005*\"time\"\n",
      "\n",
      "LSI Model of 1994:\n",
      "Topic #0: 0.253*\"people\" + 0.214*\"must\" + 0.214*\"year\" + 0.175*\"make\" + 0.175*\"new\" + 0.168*\"cut\" + 0.162*\"say\" + 0.155*\"work\" + 0.155*\"government\" + 0.149*\"care\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1995:\n",
      "Topic #0: 0.017*\"year\" + 0.014*\"people\" + 0.013*\"work\" + 0.012*\"health\" + 0.011*\"care\" + 0.008*\"congress\" + 0.006*\"job\" + 0.006*\"say\" + 0.006*\"must\" + 0.006*\"get\"\n",
      "Topic #1: 0.013*\"people\" + 0.012*\"health\" + 0.011*\"year\" + 0.010*\"care\" + 0.008*\"work\" + 0.008*\"must\" + 0.008*\"job\" + 0.007*\"time\" + 0.006*\"every\" + 0.006*\"system\"\n",
      "Topic #2: 0.015*\"health\" + 0.013*\"year\" + 0.012*\"people\" + 0.011*\"care\" + 0.010*\"work\" + 0.007*\"job\" + 0.006*\"system\" + 0.006*\"must\" + 0.006*\"one\" + 0.006*\"congress\"\n",
      "Topic #3: 0.015*\"people\" + 0.013*\"year\" + 0.012*\"health\" + 0.011*\"work\" + 0.008*\"job\" + 0.007*\"must\" + 0.006*\"care\" + 0.006*\"congress\" + 0.006*\"system\" + 0.006*\"reform\"\n",
      "Topic #4: 0.011*\"care\" + 0.011*\"people\" + 0.009*\"health\" + 0.009*\"work\" + 0.009*\"year\" + 0.006*\"congress\" + 0.006*\"welfare\" + 0.006*\"job\" + 0.005*\"must\" + 0.005*\"one\"\n",
      "\n",
      "LSI Model of 1995:\n",
      "Topic #0: 0.310*\"people\" + 0.279*\"year\" + 0.254*\"work\" + 0.248*\"health\" + 0.242*\"care\" + 0.161*\"job\" + 0.149*\"must\" + 0.136*\"congress\" + 0.124*\"welfare\" + 0.124*\"get\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1996:\n",
      "Topic #0: 0.014*\"people\" + 0.014*\"work\" + 0.011*\"government\" + 0.010*\"cut\" + 0.008*\"year\" + 0.008*\"let\" + 0.007*\"new\" + 0.007*\"give\" + 0.007*\"make\" + 0.006*\"time\"\n",
      "Topic #1: 0.013*\"people\" + 0.012*\"work\" + 0.012*\"year\" + 0.012*\"new\" + 0.011*\"government\" + 0.010*\"cut\" + 0.009*\"way\" + 0.008*\"welfare\" + 0.008*\"get\" + 0.007*\"let\"\n",
      "Topic #2: 0.015*\"year\" + 0.015*\"people\" + 0.014*\"work\" + 0.013*\"government\" + 0.012*\"new\" + 0.012*\"cut\" + 0.008*\"way\" + 0.008*\"make\" + 0.007*\"let\" + 0.007*\"get\"\n",
      "Topic #3: 0.014*\"work\" + 0.011*\"year\" + 0.010*\"cut\" + 0.010*\"new\" + 0.008*\"government\" + 0.008*\"make\" + 0.008*\"people\" + 0.007*\"let\" + 0.006*\"way\" + 0.006*\"give\"\n",
      "Topic #4: 0.016*\"work\" + 0.015*\"people\" + 0.011*\"year\" + 0.010*\"new\" + 0.009*\"let\" + 0.009*\"cut\" + 0.007*\"government\" + 0.006*\"get\" + 0.006*\"tax\" + 0.005*\"know\"\n",
      "\n",
      "LSI Model of 1996:\n",
      "Topic #0: 0.302*\"work\" + 0.277*\"people\" + 0.258*\"year\" + 0.221*\"new\" + 0.208*\"government\" + 0.208*\"cut\" + 0.164*\"let\" + 0.145*\"way\" + 0.145*\"make\" + 0.132*\"get\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1997:\n",
      "Topic #0: 0.014*\"work\" + 0.013*\"challenge\" + 0.010*\"people\" + 0.010*\"child\" + 0.008*\"congress\" + 0.008*\"family\" + 0.008*\"must\" + 0.007*\"make\" + 0.007*\"year\" + 0.006*\"community\"\n",
      "Topic #1: 0.010*\"work\" + 0.010*\"child\" + 0.009*\"challenge\" + 0.008*\"make\" + 0.007*\"people\" + 0.007*\"year\" + 0.007*\"congress\" + 0.007*\"must\" + 0.006*\"new\" + 0.006*\"every\"\n",
      "Topic #2: 0.015*\"work\" + 0.014*\"challenge\" + 0.011*\"make\" + 0.011*\"people\" + 0.011*\"child\" + 0.009*\"family\" + 0.009*\"congress\" + 0.009*\"must\" + 0.008*\"year\" + 0.007*\"new\"\n",
      "Topic #3: 0.012*\"work\" + 0.010*\"challenge\" + 0.010*\"child\" + 0.010*\"people\" + 0.009*\"year\" + 0.009*\"must\" + 0.008*\"congress\" + 0.008*\"make\" + 0.008*\"every\" + 0.006*\"family\"\n",
      "Topic #4: 0.016*\"work\" + 0.014*\"challenge\" + 0.012*\"people\" + 0.010*\"child\" + 0.009*\"make\" + 0.009*\"year\" + 0.007*\"congress\" + 0.007*\"must\" + 0.007*\"family\" + 0.006*\"one\"\n",
      "\n",
      "LSI Model of 1997:\n",
      "Topic #0: 0.335*\"work\" + 0.291*\"challenge\" + 0.240*\"people\" + 0.228*\"child\" + 0.190*\"make\" + 0.190*\"year\" + 0.177*\"must\" + 0.177*\"congress\" + 0.171*\"family\" + 0.133*\"every\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1998:\n",
      "Topic #0: 0.022*\"must\" + 0.011*\"child\" + 0.011*\"every\" + 0.009*\"school\" + 0.009*\"work\" + 0.008*\"year\" + 0.008*\"people\" + 0.007*\"new\" + 0.006*\"first\" + 0.006*\"help\"\n",
      "Topic #1: 0.019*\"must\" + 0.010*\"year\" + 0.007*\"school\" + 0.007*\"child\" + 0.007*\"community\" + 0.007*\"work\" + 0.006*\"new\" + 0.006*\"people\" + 0.006*\"world\" + 0.006*\"help\"\n",
      "Topic #2: 0.014*\"must\" + 0.008*\"every\" + 0.008*\"new\" + 0.008*\"child\" + 0.008*\"school\" + 0.008*\"year\" + 0.007*\"work\" + 0.006*\"help\" + 0.006*\"make\" + 0.005*\"challenge\"\n",
      "Topic #3: 0.018*\"must\" + 0.008*\"every\" + 0.008*\"year\" + 0.007*\"people\" + 0.007*\"child\" + 0.006*\"school\" + 0.006*\"new\" + 0.006*\"make\" + 0.006*\"work\" + 0.005*\"world\"\n",
      "Topic #4: 0.011*\"must\" + 0.010*\"year\" + 0.008*\"child\" + 0.007*\"school\" + 0.006*\"new\" + 0.006*\"every\" + 0.005*\"people\" + 0.005*\"work\" + 0.005*\"world\" + 0.005*\"parent\"\n",
      "\n",
      "LSI Model of 1998:\n",
      "Topic #0: 0.435*\"must\" + 0.233*\"year\" + 0.202*\"child\" + 0.202*\"every\" + 0.183*\"school\" + 0.177*\"new\" + 0.164*\"work\" + 0.151*\"people\" + 0.126*\"help\" + 0.120*\"make\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 1999:\n",
      "Topic #0: 0.013*\"year\" + 0.008*\"new\" + 0.007*\"first\" + 0.006*\"must\" + 0.006*\"congress\" + 0.006*\"every\" + 0.006*\"family\" + 0.006*\"child\" + 0.006*\"work\" + 0.005*\"make\"\n",
      "Topic #1: 0.008*\"child\" + 0.008*\"new\" + 0.008*\"year\" + 0.007*\"congress\" + 0.006*\"help\" + 0.006*\"work\" + 0.006*\"say\" + 0.006*\"make\" + 0.006*\"know\" + 0.006*\"first\"\n",
      "Topic #2: 0.012*\"year\" + 0.011*\"new\" + 0.006*\"child\" + 0.006*\"every\" + 0.006*\"help\" + 0.005*\"make\" + 0.005*\"work\" + 0.005*\"family\" + 0.004*\"first\" + 0.004*\"world\"\n",
      "Topic #3: 0.012*\"new\" + 0.011*\"year\" + 0.007*\"family\" + 0.007*\"make\" + 0.007*\"child\" + 0.006*\"congress\" + 0.005*\"first\" + 0.005*\"help\" + 0.005*\"work\" + 0.005*\"ask\"\n",
      "Topic #4: 0.014*\"year\" + 0.009*\"new\" + 0.008*\"congress\" + 0.007*\"child\" + 0.007*\"work\" + 0.007*\"help\" + 0.006*\"family\" + 0.006*\"every\" + 0.006*\"say\" + 0.006*\"must\"\n",
      "\n",
      "LSI Model of 1999:\n",
      "Topic #0: 0.342*\"year\" + 0.274*\"new\" + 0.198*\"child\" + 0.157*\"congress\" + 0.151*\"help\" + 0.151*\"make\" + 0.151*\"work\" + 0.144*\"first\" + 0.137*\"family\" + 0.130*\"must\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 2000:\n",
      "Topic #0: 0.009*\"must\" + 0.008*\"year\" + 0.008*\"security\" + 0.007*\"work\" + 0.006*\"school\" + 0.006*\"child\" + 0.006*\"nation\" + 0.006*\"social\" + 0.005*\"support\" + 0.005*\"century\"\n",
      "Topic #1: 0.012*\"year\" + 0.012*\"work\" + 0.011*\"must\" + 0.008*\"school\" + 0.008*\"security\" + 0.007*\"ask\" + 0.007*\"century\" + 0.007*\"social\" + 0.007*\"support\" + 0.007*\"new\"\n",
      "Topic #2: 0.010*\"must\" + 0.009*\"year\" + 0.009*\"work\" + 0.008*\"support\" + 0.007*\"security\" + 0.006*\"school\" + 0.006*\"new\" + 0.006*\"century\" + 0.006*\"social\" + 0.005*\"nation\"\n",
      "Topic #3: 0.011*\"must\" + 0.011*\"work\" + 0.011*\"year\" + 0.008*\"security\" + 0.008*\"support\" + 0.008*\"century\" + 0.007*\"social\" + 0.007*\"ask\" + 0.007*\"new\" + 0.006*\"school\"\n",
      "Topic #4: 0.011*\"must\" + 0.010*\"year\" + 0.008*\"security\" + 0.008*\"work\" + 0.008*\"congress\" + 0.008*\"support\" + 0.007*\"century\" + 0.007*\"help\" + 0.007*\"new\" + 0.006*\"school\"\n",
      "\n",
      "LSI Model of 2000:\n",
      "Topic #0: 0.282*\"year\" + 0.269*\"must\" + 0.229*\"work\" + 0.197*\"security\" + 0.177*\"support\" + 0.170*\"social\" + 0.164*\"school\" + 0.157*\"century\" + 0.144*\"new\" + 0.138*\"ask\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 2001:\n",
      "Topic #0: 0.009*\"child\" + 0.009*\"must\" + 0.008*\"make\" + 0.007*\"work\" + 0.007*\"year\" + 0.006*\"new\" + 0.005*\"let\" + 0.005*\"every\" + 0.005*\"care\" + 0.004*\"parent\"\n",
      "Topic #1: 0.013*\"year\" + 0.012*\"child\" + 0.011*\"make\" + 0.011*\"work\" + 0.011*\"must\" + 0.010*\"new\" + 0.008*\"every\" + 0.007*\"help\" + 0.007*\"care\" + 0.007*\"gun\"\n",
      "Topic #2: 0.012*\"work\" + 0.011*\"child\" + 0.011*\"must\" + 0.009*\"year\" + 0.009*\"make\" + 0.008*\"new\" + 0.006*\"help\" + 0.006*\"family\" + 0.006*\"tax\" + 0.005*\"support\"\n",
      "Topic #3: 0.014*\"year\" + 0.012*\"child\" + 0.009*\"make\" + 0.008*\"new\" + 0.008*\"work\" + 0.007*\"must\" + 0.007*\"every\" + 0.005*\"people\" + 0.005*\"school\" + 0.005*\"help\"\n",
      "Topic #4: 0.015*\"year\" + 0.011*\"new\" + 0.011*\"make\" + 0.010*\"child\" + 0.010*\"must\" + 0.010*\"work\" + 0.007*\"care\" + 0.006*\"help\" + 0.006*\"family\" + 0.006*\"let\"\n",
      "\n",
      "LSI Model of 2001:\n",
      "Topic #0: 0.285*\"year\" + 0.259*\"child\" + 0.247*\"work\" + 0.234*\"must\" + 0.228*\"make\" + 0.221*\"new\" + 0.139*\"help\" + 0.139*\"every\" + 0.133*\"care\" + 0.120*\"gun\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 2002:\n",
      "Topic #0: 0.014*\"must\" + 0.013*\"tax\" + 0.011*\"budget\" + 0.008*\"year\" + 0.007*\"pay\" + 0.006*\"child\" + 0.006*\"nation\" + 0.006*\"government\" + 0.006*\"need\" + 0.006*\"people\"\n",
      "Topic #1: 0.015*\"must\" + 0.009*\"year\" + 0.009*\"tax\" + 0.008*\"budget\" + 0.006*\"need\" + 0.006*\"pay\" + 0.006*\"security\" + 0.006*\"one\" + 0.006*\"make\" + 0.006*\"money\"\n",
      "Topic #2: 0.010*\"must\" + 0.009*\"year\" + 0.008*\"tax\" + 0.008*\"budget\" + 0.007*\"make\" + 0.007*\"one\" + 0.007*\"government\" + 0.006*\"people\" + 0.006*\"child\" + 0.006*\"pay\"\n",
      "Topic #3: 0.009*\"year\" + 0.008*\"must\" + 0.007*\"budget\" + 0.006*\"tax\" + 0.005*\"security\" + 0.005*\"money\" + 0.004*\"need\" + 0.004*\"make\" + 0.004*\"nation\" + 0.004*\"help\"\n",
      "Topic #4: 0.010*\"tax\" + 0.010*\"budget\" + 0.009*\"must\" + 0.007*\"help\" + 0.007*\"make\" + 0.006*\"pay\" + 0.006*\"year\" + 0.006*\"good\" + 0.005*\"need\" + 0.005*\"government\"\n",
      "\n",
      "LSI Model of 2002:\n",
      "Topic #0: 0.302*\"must\" + 0.238*\"tax\" + 0.222*\"year\" + 0.214*\"budget\" + 0.151*\"need\" + 0.151*\"make\" + 0.143*\"pay\" + 0.143*\"government\" + 0.135*\"one\" + 0.127*\"good\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 2003:\n",
      "Topic #0: 0.010*\"terrorist\" + 0.009*\"every\" + 0.009*\"world\" + 0.007*\"state\" + 0.007*\"country\" + 0.007*\"come\" + 0.007*\"terror\" + 0.007*\"tonight\" + 0.007*\"freedom\" + 0.006*\"many\"\n",
      "Topic #1: 0.008*\"terrorist\" + 0.008*\"world\" + 0.007*\"many\" + 0.007*\"people\" + 0.007*\"tonight\" + 0.006*\"state\" + 0.006*\"freedom\" + 0.005*\"country\" + 0.005*\"ask\" + 0.005*\"know\"\n",
      "Topic #2: 0.008*\"ask\" + 0.008*\"world\" + 0.007*\"terrorist\" + 0.007*\"state\" + 0.007*\"every\" + 0.007*\"country\" + 0.007*\"see\" + 0.006*\"tonight\" + 0.006*\"terror\" + 0.005*\"people\"\n",
      "Topic #3: 0.007*\"ask\" + 0.006*\"terrorist\" + 0.006*\"world\" + 0.006*\"every\" + 0.006*\"tonight\" + 0.005*\"many\" + 0.005*\"country\" + 0.004*\"state\" + 0.004*\"terror\" + 0.004*\"war\"\n",
      "Topic #4: 0.011*\"terrorist\" + 0.010*\"world\" + 0.008*\"country\" + 0.008*\"every\" + 0.008*\"tonight\" + 0.008*\"many\" + 0.007*\"ask\" + 0.007*\"people\" + 0.007*\"state\" + 0.006*\"freedom\"\n",
      "\n",
      "LSI Model of 2003:\n",
      "Topic #0: 0.236*\"terrorist\" + 0.223*\"world\" + 0.210*\"every\" + 0.197*\"country\" + 0.184*\"tonight\" + 0.171*\"state\" + 0.171*\"many\" + 0.171*\"ask\" + 0.158*\"people\" + 0.145*\"freedom\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n",
      "LDA Model of 2004:\n",
      "Topic #0: 0.008*\"world\" + 0.007*\"terrorist\" + 0.007*\"new\" + 0.006*\"work\" + 0.006*\"security\" + 0.006*\"nation\" + 0.006*\"must\" + 0.006*\"good\" + 0.006*\"job\" + 0.006*\"war\"\n",
      "Topic #1: 0.007*\"terrorist\" + 0.007*\"security\" + 0.005*\"must\" + 0.005*\"world\" + 0.005*\"afghanistan\" + 0.005*\"people\" + 0.005*\"terror\" + 0.005*\"work\" + 0.005*\"nation\" + 0.004*\"weapon\"\n",
      "Topic #2: 0.009*\"terrorist\" + 0.006*\"people\" + 0.006*\"security\" + 0.006*\"nation\" + 0.006*\"work\" + 0.006*\"world\" + 0.006*\"state\" + 0.006*\"freedom\" + 0.006*\"must\" + 0.005*\"regime\"\n",
      "Topic #3: 0.009*\"security\" + 0.007*\"terrorist\" + 0.007*\"must\" + 0.006*\"job\" + 0.006*\"good\" + 0.006*\"nation\" + 0.005*\"new\" + 0.005*\"people\" + 0.005*\"work\" + 0.005*\"freedom\"\n",
      "Topic #4: 0.007*\"terrorist\" + 0.007*\"security\" + 0.006*\"nation\" + 0.006*\"must\" + 0.006*\"freedom\" + 0.006*\"world\" + 0.005*\"good\" + 0.005*\"people\" + 0.005*\"terror\" + 0.005*\"country\"\n",
      "\n",
      "LSI Model of 2004:\n",
      "Topic #0: 0.207*\"terrorist\" + 0.196*\"world\" + 0.196*\"security\" + 0.186*\"must\" + 0.165*\"nation\" + 0.155*\"work\" + 0.145*\"good\" + 0.145*\"freedom\" + 0.145*\"people\" + 0.134*\"new\"\n",
      "Topic #1: \n",
      "Topic #2: \n",
      "Topic #3: \n",
      "Topic #4: \n"
     ]
    },
    {
     "data": {
      "text/plain": "[(<gensim.models.ldamodel.LdaModel at 0x2308f1e2d70>,\n  <gensim.models.lsimodel.LsiModel at 0x230b2698820>),\n (<gensim.models.ldamodel.LdaModel at 0x230b269a500>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269b340>),\n (<gensim.models.ldamodel.LdaModel at 0x230b269a1a0>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269a170>),\n (<gensim.models.ldamodel.LdaModel at 0x230b2699b10>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269a020>),\n (<gensim.models.ldamodel.LdaModel at 0x230b2698d90>,\n  <gensim.models.lsimodel.LsiModel at 0x230b2699db0>),\n (<gensim.models.ldamodel.LdaModel at 0x230b2698fd0>,\n  <gensim.models.lsimodel.LsiModel at 0x230b2699000>),\n (<gensim.models.ldamodel.LdaModel at 0x230b269bc40>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269bc10>),\n (<gensim.models.ldamodel.LdaModel at 0x230b2698430>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269ae30>),\n (<gensim.models.ldamodel.LdaModel at 0x230b2698790>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269a2f0>),\n (<gensim.models.ldamodel.LdaModel at 0x230b269a7d0>,\n  <gensim.models.lsimodel.LsiModel at 0x230b26985e0>),\n (<gensim.models.ldamodel.LdaModel at 0x230b269a320>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269a7a0>),\n (<gensim.models.ldamodel.LdaModel at 0x230b269aa10>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269aad0>),\n (<gensim.models.ldamodel.LdaModel at 0x230b2699ba0>,\n  <gensim.models.lsimodel.LsiModel at 0x230b2699f00>),\n (<gensim.models.ldamodel.LdaModel at 0x230b2699a50>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269bd30>),\n (<gensim.models.ldamodel.LdaModel at 0x230b269b370>,\n  <gensim.models.lsimodel.LsiModel at 0x230b269bd00>)]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim import models, corpora\n",
    "\n",
    "df['tokens'] = tokenized_speeches\n",
    "\n",
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(df.tokens)\n",
    "# dictionary.filter_extremes(no_below=3, no_above=.03)\n",
    "\n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in df.tokens]\n",
    "\n",
    "def sotu_topic_finder(year):\n",
    "    \"\"\"\n",
    "    Find SOTU topics using LDA. The LDA model is only trained on the text of that year topic\n",
    "    Input: index i of the speech\n",
    "    Output: list 5 topics found by the model\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    sent_text = sent_tokenize(df.speech[year - 1790])\n",
    "    token_list = []\n",
    "    for sent in sent_text:\n",
    "        cleaned_sent = clean_lemma_text(sent)\n",
    "        token_list.append(cleaned_sent)\n",
    "\n",
    "    # Prepare the dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(token_list)\n",
    "    corpus = [dictionary.doc2bow(text) for text in token_list]\n",
    "\n",
    "    # Build the LDA model\n",
    "    lda_model = models.LdaModel(corpus=corpus,\n",
    "                                num_topics=10,\n",
    "                                id2word=dictionary)\n",
    "\n",
    "    # Output model\n",
    "    print(\"LDA Model of %i:\" % year)\n",
    "    for idx in range(5):\n",
    "        # Print the first 10 most representative topics\n",
    "        print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
    "\n",
    "    # build the LSI model\n",
    "    lsi_model = models.LsiModel(corpus=corpus,\n",
    "                                num_topics=10,\n",
    "                                id2word=dictionary)\n",
    "\n",
    "    print()\n",
    "    print(\"LSI Model of %i:\" % year)\n",
    "    for idx in range(5):\n",
    "        # Print the first 10 most representative topics\n",
    "        print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10))\n",
    "\n",
    "    return lda_model, lsi_model\n",
    "\n",
    "\n",
    "yearly_models = []\n",
    "for year in range(1990, 2005):\n",
    "    yearly_models.append(sotu_topic_finder(year))\n",
    "\n",
    "yearly_models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# visualize topics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
