{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "1000000000"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "csv.field_size_limit(1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the file and performing basic cleanup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\3938583509.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['speech'] = df['speech'].str.replace('\\nAddress on Administration Goals (Budget Message)\\n', '')\n",
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\3938583509.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['date'][df['date'] == 'Address on Administration Goals (Budget Message)'] = temp_date.values[0]\n",
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\3938583509.py:25: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['speech'] = df['speech'].str.replace('\\\\\\'', '')\n",
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\3938583509.py:29: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['speech'] = df['speech'].str.replace('[{}]'.format(string.punctuation), '')\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# read csv file from data folder\n",
    "df = pd.read_csv(os.path.join('data', 'state-of-the-union.csv'), names=['year', 'speech'], skiprows=1)\n",
    "\n",
    "df['speech'] = df['speech'].str.replace('\\nState of the Union Address\\n', '')\n",
    "df['speech'] = df['speech'].str.replace('\\nAddress to Joint Session of Congress \\n', '')\n",
    "df['speech'] = df['speech'].str.replace('\\nAddress on Administration Goals (Budget Message)\\n', '')\n",
    "df['speech'] = df['speech'].str.replace('\\nAddress on Administration Goals\\n', '')\n",
    "df['speech'] = df['speech'].str.replace('\\nAddress to Congress \\n', '')\n",
    "\n",
    "df['president'] = df['speech']\n",
    "\n",
    "df['president'] = df['president'].str.split('\\n').str[0]\n",
    "df['date'] = df['speech'].str.split('\\n').str[1]\n",
    "\n",
    "temp_date = df[df['date'] == 'Address on Administration Goals (Budget Message)']['speech'].str.split('\\n').str[3]\n",
    "df['date'][df['date'] == 'Address on Administration Goals (Budget Message)'] = temp_date.values[0]\n",
    "\n",
    "# delete first 3 lines of speech\n",
    "df['speech'] = df['speech'].str.split('\\n').str[3:]\n",
    "# make a string list\n",
    "df['speech'] = df['speech'].str.join(' ')\n",
    "# replace \\ with ''\n",
    "df['speech'] = df['speech'].str.replace('\\\\\\'', '')\n",
    "# decapitalize\n",
    "df['speech'] = df['speech'].str.lower()\n",
    "# remove punctuation\n",
    "df['speech'] = df['speech'].str.replace('[{}]'.format(string.punctuation), '')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performing Lemmatization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in word_tokenize(text)]\n",
    "\n",
    "\n",
    "df['speech'] = df['speech'].apply(lemmatize_text)\n",
    "df['speech'] = df['speech'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df['speech'] = df['speech'].apply(lambda x: ' '.join(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performing Stemming"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellowcitizen senat hou repr meet feel much sa...\n",
      "1    fellowcitizen senat hou repr vain may expect p...\n",
      "2    fellowcitizen senat hou repr abat satisfact me...\n",
      "3    fellowcitizen senat hou repr sinc commenc term...\n",
      "4    fellowcitizen senat hou repr call mind graciou...\n",
      "5    fellowcitizen senat hou repr trust deceiv indu...\n",
      "6    fellowcitizen senat hou repr recur intern situ...\n",
      "7    gentlemen senat gentlemen hou repr wa time app...\n",
      "8    gentlemen senat gentlemen hou repr rever resig...\n",
      "9    gentlemen senat gentlemen hou repr peculiar sa...\n",
      "Name: speech, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# # perform stemming\n",
    "# stemmer = PorterStemmer()\n",
    "#\n",
    "#\n",
    "# def stem_text(text):\n",
    "#     return [stemmer.stem(w) for w in word_tokenize(text)]\n",
    "#\n",
    "#\n",
    "# df['speech'] = df['speech'].apply(stem_text)\n",
    "# df['speech'] = df['speech'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# df['speech'] = df['speech'].apply(lambda x: ' '.join(x))\n",
    "#\n",
    "# # remove punctuation\n",
    "# # import string\n",
    "# #\n",
    "# # df['speech'] = df['speech'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "\n",
    "\n",
    "print(df['speech'].head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellowcitizen senat hous repres  meet feel muc...\n",
      "1    fellowcitizen senat hous repres   vain may exp...\n",
      "2    fellowcitizen senat hous repres  abat satisfac...\n",
      "3    fellowcitizen senat hous repres  sinc commenc ...\n",
      "4    fellowcitizen senat hous repres  call mind gra...\n",
      "5    fellowcitizen senat hous repres  trust deceiv ...\n",
      "6    fellowcitizen senat hous repres  recur intern ...\n",
      "7    gentlemen senat gentlemen hous repres  wa time...\n",
      "8    gentlemen senat gentlemen hous repres  rever r...\n",
      "9    gentlemen senat gentlemen hous repres  peculia...\n",
      "Name: speech, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_18900\\939221554.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['speech'] = df['speech'].str.replace('[{}]'.format(string.punctuation), '')\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "import string\n",
    "\n",
    "df['speech'] = df['speech'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "print(df['speech'].head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [7], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m corpora\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# create a dictionary from a list of speeches\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m dictionary \u001B[38;5;241m=\u001B[39m \u001B[43mcorpora\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDictionary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mspeech\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# convert the dictionary to a bag of words\u001B[39;00m\n\u001B[0;32m      7\u001B[0m corpus \u001B[38;5;241m=\u001B[39m [dictionary\u001B[38;5;241m.\u001B[39mdoc2bow(speech) \u001B[38;5;28;01mfor\u001B[39;00m speech \u001B[38;5;129;01min\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspeech\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "File \u001B[1;32m~\\PycharmProjects\\Aritificial Intelligence\\Assignment 2\\venv\\lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001B[0m, in \u001B[0;36mDictionary.__init__\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_nnz \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m documents \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprune_at\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprune_at\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_lifecycle_event(\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreated\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     81\u001B[0m         msg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_docs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m documents (total \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_pos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m corpus positions)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     82\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Aritificial Intelligence\\Assignment 2\\venv\\lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001B[0m, in \u001B[0;36mDictionary.add_documents\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m    201\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madding document #\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m to \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, docno, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;66;03m# update Dictionary with the document\u001B[39;00m\n\u001B[1;32m--> 204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdoc2bow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_update\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# ignore the result, here we only care about updating token ids\u001B[39;00m\n\u001B[0;32m    206\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m documents (total \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m corpus positions)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_docs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_pos)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Aritificial Intelligence\\Assignment 2\\venv\\lib\\site-packages\\gensim\\corpora\\dictionary.py:241\u001B[0m, in \u001B[0;36mDictionary.doc2bow\u001B[1;34m(self, document, allow_update, return_missing)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;124;03m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \n\u001B[0;32m    211\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    238\u001B[0m \n\u001B[0;32m    239\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(document, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoc2bow expects an array of unicode tokens on input, not a single string\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    243\u001B[0m \u001B[38;5;66;03m# Construct (word, frequency) mapping.\u001B[39;00m\n\u001B[0;32m    244\u001B[0m counter \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mint\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# create a dictionary from a list of speeches\n",
    "dictionary = corpora.Dictionary(df['speech'])\n",
    "\n",
    "# convert the dictionary to a bag of words\n",
    "corpus = [dictionary.doc2bow(speech) for speech in df['speech']]\n",
    "\n",
    "print(corpus[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['value', 'trying', 'copy', 'slice', 'from', 'dataframe', 'caveats', 'documentation']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "tokens = gensim.utils.simple_preprocess(\"A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation\", min_len=4)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords list\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "# Thank you to kaggle user mjmurphy28 for this code\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', '--', '-','...', 'american', 'america', 'world']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "\n",
    "# Lemmatize the stop words\n",
    "tokenizer=LemmaTokenizer()\n",
    "D = tokenizer(' '.join(stop_words))\n",
    "documents = df['speech']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lemmatize with POS tagging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\fazal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#Return pos tag in wordnetlemmatizer format\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Create the lemmatoken\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class POSLemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', '--', '-', '...', 'american', 'america', 'world']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t, get_wordnet_pos(t)) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "\n",
    "# Lemmatize the stop words\n",
    "pos_tokenizer=POSLemmaTokenizer()\n",
    "pos_token_stop=pos_tokenizer(' '.join(stop_words))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Creating Tfidf on no-POS tagged documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\PycharmProjects\\Aritificial Intelligence\\Assignment 2\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 225 entries, 0 to 224\n",
      "Columns: 22564 entries, 000 to zooming\n",
      "dtypes: float64(22564)\n",
      "memory usage: 38.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Create TF-idf model\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop,\n",
    "                              tokenizer=tokenizer)\n",
    "#Fit transform current document\n",
    "tfidf_doc = vectorizer.fit_transform(documents)\n",
    "\n",
    "tfidf_array = tfidf_doc.toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_array, columns = vectorizer.get_feature_names())\n",
    "tfidf_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating TFIDF with POS tagged document"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\PycharmProjects\\Aritificial Intelligence\\Assignment 2\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 225 entries, 0 to 224\n",
      "Columns: 19696 entries, 000 to zoom\n",
      "dtypes: float64(19696)\n",
      "memory usage: 33.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Create TF-idf model\n",
    "pos_vectorizer = TfidfVectorizer(stop_words=pos_token_stop,\n",
    "                              tokenizer=pos_tokenizer)\n",
    "#Fit transform current document\n",
    "pos_tfidf_doc = pos_vectorizer.fit_transform(documents)\n",
    "\n",
    "pos_tfidf_array = pos_tfidf_doc.toarray()\n",
    "pos_tfidf_df = pd.DataFrame(pos_tfidf_array, columns = pos_vectorizer.get_feature_names())\n",
    "pos_tfidf_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA on no-POS taggged docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\4183290690.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['topic'][i] = topc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [requisite, militia, object, u, attachment]\n",
      "1           [object, may, subscription, united, state]\n",
      "2           [chickamauga, tribe, law, upon, provision]\n",
      "3              [shall, relate, warmest, united, state]\n",
      "4    [inspector, insurrection, state, pennsylvania,...\n",
      "5    [country, situation, tranquillity, gentleman, ...\n",
      "6              [may, war, united, commissioner, state]\n",
      "7     [gentleman, treaty, commissioner, united, state]\n",
      "8      [state, gentleman, france, croix, commissioner]\n",
      "9    [punctuality, united, state, philadelphia, gen...\n",
      "Name: topic, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['topic'] = np.arange(0,225)\n",
    "\n",
    "#Print out the topic-representing words in each year SOTU speech:\n",
    "for i in range(0, 225):\n",
    "    topc = []\n",
    "    topic_words = tfidf_df.iloc[i, :].sort_values().tail(5).reset_index()\n",
    "    for word in topic_words['index']:\n",
    "        topc.append(word)\n",
    "    df['topic'][i] = topc\n",
    "\n",
    "print(df['topic'].head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA on POS-tagged docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fazal\\AppData\\Local\\Temp\\ipykernel_5008\\728159892.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pos_topic'][i] = topcs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               [militia, object, u, make, attachment]\n",
      "1           [object, may, subscription, united, state]\n",
      "2           [chickamauga, tribe, law, upon, provision]\n",
      "3             [militia, warlike, shall, united, state]\n",
      "4    [inspector, insurrection, state, pennsylvania,...\n",
      "5    [country, situation, tranquillity, gentleman, ...\n",
      "6              [may, war, united, commissioner, state]\n",
      "7     [gentleman, treaty, commissioner, united, state]\n",
      "8      [state, gentleman, france, croix, commissioner]\n",
      "9    [punctuality, united, state, philadelphia, gen...\n",
      "Name: pos_topic, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['pos_topic'] = np.arange(0,225)\n",
    "\n",
    "#Print out the topic-representing words in each year SOTU speech:\n",
    "for i in range(0, 225):\n",
    "    topcs = []\n",
    "    topic_words = pos_tfidf_df.iloc[i, :].sort_values().tail(5).reset_index()\n",
    "    for word in topic_words['index']:\n",
    "        topcs.append(word)\n",
    "    df['pos_topic'][i] = topcs\n",
    "\n",
    "print(df['pos_topic'].head(10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790 1790\n",
      "1791 1791\n",
      "1792 1792\n",
      "1793 1793\n",
      "1794 1794\n",
      "1795 1795\n",
      "1796 1796\n",
      "1797 1797\n",
      "1798 1798\n",
      "1799 1799\n",
      "1800 1800\n",
      "1801 1801\n",
      "1802 1802\n",
      "1803 1803\n",
      "1804 1804\n",
      "1805 1805\n",
      "1806 1806\n",
      "1807 1807\n",
      "1808 1808\n",
      "1809 1809\n",
      "1810 1810\n",
      "1811 1811\n",
      "1812 1812\n",
      "1813 1813\n",
      "1814 1814\n",
      "1815 1815\n",
      "1816 1816\n",
      "1817 1817\n",
      "1818 1818\n",
      "1819 1819\n",
      "1820 1820\n",
      "1821 1821\n",
      "1822 1822\n",
      "1823 1823\n",
      "1824 1824\n",
      "1825 1825\n",
      "1826 1826\n",
      "1827 1827\n",
      "1828 1828\n",
      "1829 1829\n",
      "1830 1830\n",
      "1831 1831\n",
      "1832 1834\n",
      "1833 1832\n",
      "1834 1834\n",
      "1835 1835\n",
      "1836 1836\n",
      "1837 1837\n",
      "1838 1838\n",
      "1839 1839\n",
      "1840 1840\n",
      "1841 1841\n",
      "1842 1842\n",
      "1843 1843\n",
      "1844 1844\n",
      "1845 1845\n",
      "1846 1846\n",
      "1847 1847\n",
      "1848 1848\n",
      "1849 1849\n",
      "1850 1850\n",
      "1851 1851\n",
      "1852 1852\n",
      "1853 1853\n",
      "1854 1854\n",
      "1855 1855\n",
      "1856 1856\n",
      "1857 1857\n",
      "1858 1858\n",
      "1859 1859\n",
      "1860 1860\n",
      "1861 1861\n",
      "1862 1862\n",
      "1863 1863\n",
      "1864 1864\n",
      "1865 1865\n",
      "1866 1866\n",
      "1867 1867\n",
      "1868 1868\n",
      "1869 1869\n",
      "1870 1870\n",
      "1871 1871\n",
      "1872 1872\n",
      "1873 1873\n",
      "1874 1874\n",
      "1875 1875\n",
      "1876 1876\n",
      "1877 1877\n",
      "1878 1878\n",
      "1879 1879\n",
      "1880 1880\n",
      "1881 1881\n",
      "1882 1882\n",
      "1883 1883\n",
      "1884 1884\n",
      "1885 1885\n",
      "1886 1886\n",
      "1887 1887\n",
      "1888 1888\n",
      "1889 1889\n",
      "1890 1890\n",
      "1891 1891\n",
      "1892 1892\n",
      "1893 1893\n",
      "1894 1894\n",
      "1895 1895\n",
      "1896 1896\n",
      "1897 1897\n",
      "1898 1898\n",
      "1899 1899\n",
      "1900 1900\n",
      "1901 1901\n",
      "1902 1902\n",
      "1903 1903\n",
      "1904 1905\n",
      "1905 1905\n",
      "1906 1906\n",
      "1907 1907\n",
      "1908 1908\n",
      "1909 1909\n",
      "1910 1912\n",
      "1911 1911\n",
      "1912 1912\n",
      "1913 1913\n",
      "1914 1914\n",
      "1915 1915\n",
      "1916 1916\n",
      "1917 1917\n",
      "1918 1918\n",
      "1919 1919\n",
      "1920 1920\n",
      "1921 1921\n",
      "1922 1922\n",
      "1923 1923\n",
      "1924 1924\n",
      "1925 1925\n",
      "1926 1926\n",
      "1927 1927\n",
      "1928 1928\n",
      "1929 1929\n",
      "1930 1930\n",
      "1931 1931\n",
      "1932 1932\n",
      "1933 1934\n",
      "1934 1935\n",
      "1935 1936\n",
      "1936 1937\n",
      "1937 1938\n",
      "1938 1939\n",
      "1939 1940\n",
      "1940 1941\n",
      "1941 1942\n",
      "1942 1943\n",
      "1943 1944\n",
      "1944 1945\n",
      "1945 1946\n",
      "1946 1947\n",
      "1947 1948\n",
      "1948 1949\n",
      "1949 1950\n",
      "1950 1951\n",
      "1951 1951\n",
      "1952 1953\n",
      "1953 1953\n",
      "1954 1954\n",
      "1955 1955\n",
      "1956 1956\n",
      "1957 1957\n",
      "1958 1958\n",
      "1959 1959\n",
      "1960 1960\n",
      "1961 1961\n",
      "1962 1961\n",
      "1963 1962\n",
      "1964 1963\n",
      "1965 1964\n",
      "1966 1965\n",
      "1967 1966\n",
      "1968 1967\n",
      "1969 1968\n",
      "1970 1969\n",
      "1971 1970\n",
      "1972 1971\n",
      "1973 1972\n",
      "1974 1973\n",
      "1975 1974\n",
      "1976 1975\n",
      "1977 1976\n",
      "1978 1977\n",
      "1979 1978\n",
      "1980 1979\n",
      "1981 1980\n",
      "1982 1981\n",
      "1983 1982\n",
      "1984 1983\n",
      "1985 1984\n",
      "1986 1985\n",
      "1987 1986\n",
      "1988 1987\n",
      "1989 1988\n",
      "1990 1989\n",
      "1991 1990\n",
      "1992 1991\n",
      "1993 1992\n",
      "1994 1993\n",
      "1995 1994\n",
      "1996 1995\n",
      "1997 1996\n",
      "1998 1997\n",
      "1999 1998\n",
      "2000 1999\n",
      "2001 2000\n",
      "2002 2001\n",
      "2003 2001\n",
      "2004 2002\n",
      "2005 2003\n",
      "2006 2004\n",
      "2007 2005\n",
      "2008 2006\n",
      "2009 2007\n",
      "2010 2008\n",
      "2011 2009\n",
      "2012 2010\n",
      "2013 2011\n",
      "2014 2012\n",
      "[1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1905, 1906, 1907, 1908, 1909, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(df['year'].values)):\n",
    "    print(i+1790, df['year'][i])\n",
    "\n",
    "set1 = set(df['year'].values)\n",
    "print(list(set1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
